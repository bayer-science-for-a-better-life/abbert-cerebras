#RoBERTa-Base for heavy chain dataset

### Input
train_input:
    data_processor: OasMlmOnlyTfRecordsDynamicMaskProcessor
    data_dir: [
        "/cb/customers/poc/bayer_data/tfrecords/bayer_tfrecords/paired/*heavy_lte2017*.tfrecord",
        "/cb/customers/poc/bayer_data/tfrecords/bayer_tfrecords/unpaired/*heavy_lte2017*.tfrecord",
    ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    shuffle: True
    shuffle_seed: 0
    repeat: True
    batch_size: 1024
    dummy_vocab_size: 32
    # fw_masked_lm_prob: [0.1, 0.2, 0.13, 0.12]
    cdr_masked_lm_prob: [0.25, 0.25, 0.5]
    subregion: "cdr"
    subregion_overlap: 2



eval_input:
    data_processor: OasMlmOnlyTfRecordsDynamicMaskProcessor
    data_dir: [
        "/cb/ml/language/datasets/bayer_tfrecords/paired/*heavy_eq2018*.tfrecord",
        "/cb/ml/language/datasets/bayer_tfrecords/unpaired/*heavy_eq2018*.tfrecord"
    ]
    # # Test dataset:
    # data_dir: [
    #     "/cb/ml/language/datasets/bayer_tfrecords/paired/*heavy_gte2019*.tfrecord",
    #     "/cb/ml/language/datasets/bayer_tfrecords/unpaired/*heavy_gte2019*.tfrecord"
    # ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    batch_size: 16
    dummy_vocab_size: 32
    # fw_masked_lm_prob: [0.1, 0.2, 0.13, 0.12]
    cdr_masked_lm_prob: [0.25, 0.25, 0.5]
    subregion: "cdr"
    subregion_overlap: 2
    shuffle: False
    shuffle_seed: 0
    repeat: False


predict_input:
    data_processor: OasMlmOnlyTfRecordsPredictProcessor
    data_dir: [
        "/cb/home/aarti/ws/code/bayer_modelzoo/bayer_shared/bert/tf/test_tfrecords_fldr/paired/*heavy_gte2019*.tfrecord",
    ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    batch_size: 16
    dummy_vocab_size: 32
    shuffle: False
    shuffle_seed: 0
    repeat: False
    # fw_masked_lm_prob: [0.1, 0.2, 0.13, 0.12]
    cdr_masked_lm_prob: [0.25, 0.25, 0.5]
    subregion: "cdr"
    subregion_overlap: 2


model:
    # Embedding
    hidden_size: 768
    use_position_embedding: True
    use_segment_embedding: False
    position_embedding_type: 'learned' # {'learned', 'fixed'}
    max_position_embeddings: 512
    share_embedding_weights: True

    # Encoder
    num_hidden_layers: 12
    dropout_rate: 0.1
    layer_norm_epsilon: 1e-5

    # Encoder - Attention
    num_heads: 12
    attention_dropout_rate: 0.1

    # Encoder - ffn
    filter_size: 3072
    encoder_nonlinearity: 'gelu'
    use_ffn_bias: True

    # Task-specific
    disable_nsp: True
    mlm_loss_weight: 1  # average length of heavy is 109.93651102662263

    use_vsl: False
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    all_encoder_outputs: True

### Optimization
optimizer:
    optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
    weight_decay_rate: 0.01
    epsilon: 1e-6
    max_gradient_norm: 1.0
    disable_lr_steps_reset: True
    learning_rate:
        - steps: 9037829
          scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.0006
        - scheduler: "Linear"
          initial_learning_rate: 0.0006
          end_learning_rate: 0.0
          steps: 90378294
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Cerebras parameters
runconfig:
    max_steps: 90378294
    save_summary_steps: 100
    save_checkpoints_steps: 10000
    keep_checkpoint_max: 10
    tf_random_seed: 1235
    enable_distributed: False
