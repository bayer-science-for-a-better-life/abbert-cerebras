#RoBERTa-Base for heavy chain dataset

### Input
train_input:
    data_processor: OasMlmOnlyTfRecordsDynamicMaskProcessor
    data_dir: [
        "/cb/customers/poc/bayer_data/bayer_small_sample_tfrecords/paired/*heavy_lte2017*.tfrecord",
        "/cb/customers/poc/bayer_data/bayer_small_sample_tfrecords/unpaired/*heavy_lte2017*.tfrecord",
    ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    shuffle: True
    shuffle_seed: 0
    repeat: True
    batch_size: 1024
    dummy_vocab_size: 32
    # fw_masked_lm_prob: [0.1, 0.2, 0.13, 0.12]
    # cdr_masked_lm_prob: [0.1, 0.3, 0.05]


eval_input:
    data_processor: OasMlmOnlyTfRecordsDynamicMaskProcessor
    data_dir: [
        "/cb/ml/language/datasets/bayer_small_sample_tfrecords/paired/*heavy_lte2017*.tfrecord",
        "/cb/ml/language/datasets/bayer_small_sample_tfrecords/unpaired/*heavy_lte2017*.tfrecord",
    ]
    # # Test dataset:
    # data_dir: [
    #     "/cb/ml/language/datasets/bayer_small_sample_tfrecords/paired/*heavy_gte2019*.tfrecord",
    #     "/cb/ml/language/datasets/bayer_small_sample_tfrecords/unpaired/*heavy_gte2019*.tfrecord"
    # ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    batch_size: 16
    dummy_vocab_size: 32
    # fw_masked_lm_prob: [0.1, 0.2, 0.13, 0.12]
    # cdr_masked_lm_prob: [0.1, 0.3, 0.05]
    shuffle: False
    shuffle_seed: 0
    repeat: False


predict_input:
    data_processor: OasMlmOnlyTfRecordsPredictProcessor
    data_dir: [
        "/cb/ml/language/datasets/bayer_small_sample_tfrecords/paired/*heavy_lte2017*.tfrecord",
        "/cb/ml/language/datasets/bayer_small_sample_tfrecords/unpaired/*heavy_lte2017*.tfrecord",
    ]

    max_sequence_length: 164
    max_predictions_per_seq: 26
    batch_size: 16
    dummy_vocab_size: 32
    shuffle: False
    shuffle_seed: 0
    repeat: False


model:
    # Embedding
    hidden_size: 768
    use_position_embedding: True
    use_segment_embedding: False
    position_embedding_type: 'learned' # {'learned', 'fixed'}
    max_position_embeddings: 512
    share_embedding_weights: True

    # Encoder
    num_hidden_layers: 12
    dropout_rate: 0.1
    layer_norm_epsilon: 1e-5

    # Encoder - Attention
    num_heads: 12
    attention_dropout_rate: 0.1

    # Encoder - ffn
    filter_size: 3072
    encoder_nonlinearity: 'gelu'
    use_ffn_bias: True

    # Task-specific
    disable_nsp: True
    mlm_loss_weight: 1  # average length of heavy is 109.93651102662263

    use_vsl: False
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    all_encoder_outputs: True

### Optimization
optimizer:
    optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
    weight_decay_rate: 0.01
    epsilon: 1e-6
    max_gradient_norm: 1.0
    disable_lr_steps_reset: True
    learning_rate:
        - steps: 1057
          scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.0001
        - scheduler: "Linear"
          initial_learning_rate: 0.0001
          end_learning_rate: 0.0
          steps: 10572
    # learning_rate:
    #     # Paper says heavy is 166 epochs with 1e-4 and remaining 534 epochs at 1e-3
    #     - steps: 21432567
    #       scheduler: "Constant"
    #       learning_rate: 0.0001
    #     - scheduler: "Constant"
    #       learning_rate: 0.001
    #       steps: 68945727
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Cerebras parameters
runconfig:
    max_steps: 10572
    save_summary_steps: 100
    save_checkpoints_steps: 200
    keep_checkpoint_max: 10
    tf_random_seed: 1235
    enable_distributed: False
