#RoBERTa-Base

### Input
train_input:
    data_processor: OasMlmOnlyTfRecordsDynamicMaskProcessor
    data_dir: [
        "/cb/customers/poc/bayer_data/tfrecords/bayer_tfrecords/paired/*heavy_lte2017*.tfrecord",
        "/cb/customers/poc/bayer_data/tfrecords/bayer_tfrecords/unpaired/*heavy_lte2017*.tfrecord",
    ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    shuffle: True
    shuffle_seed: 0
    repeat: True
    batch_size: 1024
    dummy_vocab_size: 32
    shuffle_buffer: 500000
    cdr_masked_lm_prob: [0.25, 0.25, 0.5]
    subregion: cdr
    subregion_overlap: 2


eval_input:
    data_processor: OasMlmOnlyTfRecordsDynamicMaskProcessor
    data_dir: [
        "/cb/ml/language/datasets/bayer_tfrecords/paired/*heavy_eq2018*.tfrecord",
        "/cb/ml/language/datasets/bayer_tfrecords/unpaired/*heavy_eq2018*.tfrecord",
    ]
    #### Test dataset:
    # data_dir: [
    #     "/cb/customers/poc/bayer_data/bayer_20211202/bayer_filter_default_20211202_tfrecs_hash/paired/*test_10*.tfrecord",
    #     "/cb/customers/poc/bayer_data/bayer_20211202/bayer_filter_default_20211202_tfrecs_hash/unpaired/*test_10*.tfrecord"
    # ]
    max_sequence_length: 164
    max_predictions_per_seq: 26
    batch_size: 1024
    dummy_vocab_size: 32
    shuffle_buffer: 500000
    cdr_masked_lm_prob: [0.25, 0.25, 0.5]
    subregion: cdr
    subregion_overlap: 2

predict_input:
    max_sequence_length: 164
    dummy_vocab_size: 32
    vocab_type: "vocab_30"

model:
    # Embedding
    hidden_size: 768
    use_position_embedding: True
    use_segment_embedding: False
    position_embedding_type: 'learned' # {'learned', 'fixed'}
    max_position_embeddings: 512
    share_embedding_weights: True

    # Encoder
    num_hidden_layers: 12
    dropout_rate: 0.1
    layer_norm_epsilon: 1e-5

    # Encoder - Attention
    num_heads: 12
    attention_dropout_rate: 0.1

    # Encoder - ffn
    filter_size: 3072
    encoder_nonlinearity: 'gelu'
    use_ffn_bias: True

    # Task-specific
    disable_nsp: True
    mlm_loss_weight: 1

    use_vsl: False
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    all_encoder_outputs: True


### Optimization
optimizer:
    optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
    weight_decay_rate: 0.01
    epsilon: 1e-6
    max_gradient_norm: 1.0
    disable_lr_steps_reset: True
    learning_rate:
        - steps: 9037829
          scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.0004
        - scheduler: "Linear"
          initial_learning_rate: 0.0004
          end_learning_rate: 0.0
          steps: 90378294
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Cerebras parameters
runconfig:
    max_steps: 90378294
    save_summary_steps: 100
    save_checkpoints_steps: 100000
    keep_checkpoint_max: 0
    tf_random_seed: 1235
    enable_distributed: False
